---
title: "Crew AI Integration"
sidebarTitle: "Crew AI"
description: "Integrate Helicone with Crew AI, a multi-agent framework for automating complex workflows. Monitor AI-driven tasks and agent interactions for real use cases."
twitter:title: "Crew AI Integration - Helicone OSS LLM Observability"
icon: "cubes"
iconType: "solid"
---

## Introduction

[Crew AI](https://github.com/joaomdmoura/crewAI) is a multi-agent framework that simplifies the automation of complex workflows. By integrating [Helicone](https://www.helicone.ai) with Crew AI, you can gain real-time observability into your agents’ prompts, responses, and usage analytics.

This guide walks you through setting up Helicone alongside Crew AI, demonstrating how to leverage **Helicone Headers** for session tracking, caching, and structured prompt monitoring.

## Integration Steps

<Steps>
  <Step title="1. Create an Account & Generate an API Key">
    1. Log into [Helicone](https://www.helicone.ai) (or create a new account).
    2. Generate a [write-only API key](https://docs.helicone.ai/helicone-headers/helicone-auth).

    <Note>
      We strongly recommend storing your Helicone API key in an environment variable for security.  
      For example, set <code>HELICONE_API_KEY</code> in your system environment and retrieve it in your code.
    </Note>
  </Step>

  <Step title="2. Configure Helicone">
    Configure your environment to securely store and access your API keys.

    <CodeGroup>
    ```python example.py
    import os

    # It's a best practice to store API keys in environment variables
    os.environ["HELICONE_API_KEY"] = "your-helicone-api-key"
    os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
    ```
    </CodeGroup>

    <Note>
      Ensure that your environment variables are securely managed and not hard-coded into your source files. Tools like [dotenv](https://github.com/motdotla/dotenv) can help manage environment variables effectively.
    </Note>
  </Step>

  <Step title="3. Integrate Helicone with Crew AI">
    Below is a sample code snippet demonstrating how to integrate Helicone into Crew AI via the <code>LLM</code> class. Replace the placeholder values (e.g., <code>dummy</code> or <code>Bearer KEY</code>) with your actual credentials and environment variables.

    <CodeGroup>
    ```python example.py
    import os
    from crewai import LLM

    # Instantiate a Helicone-enabled LLM
    helicone_llm = LLM(
        model="gpt-4o-mini",  # Use your preferred model
        temperature=0.7,
        base_url="https://oai.helicone.ai/v1",   # Helicone endpoint
        api_key=os.environ.get("OPENAI_API_KEY", "dummy"),  # LLM provider API key
        extra_headers={
            "Helicone-Auth": "Bearer " + os.environ.get("HELICONE_API_KEY", ""),  # Helicone Auth
            "Helicone-Session-Id": "Unique session/tracing ID",
            "Helicone-Session-Name": "Descriptive session name, e.g., Course Plan",
            "Helicone-Session-Path": "parent/child",  # Nested trace paths
            "Helicone-Cache-Enabled": "true",  # or "false" if you do not want caching
            "Helicone-Prompt-Id": "prompt_story",
            # ... add additional headers as needed
        },
    )
    ```
    </CodeGroup>

    <Note>
      - When using **prompt templates**, ensure your variables appear in the <code>goal</code>, <code>role</code>, or <code>backstory</code> strings.  
      - Refer to [Helicone's Prompt Management docs](https://docs.helicone.ai/features/prompts) for more details on properly structuring prompt inputs.  
      - Explore all available headers and their features in the [Helicone Header Directory](https://docs.helicone.ai/helicone-headers/header-directory).
    </Note>
  </Step>

  <Step title="4. Incorporate the Agent Class">
    Assign the configured `LLM` to a Crew AI `Agent`. This step involves defining the agent's role, goal, and backstory, utilizing prompt templates with **Helicone Prompt Inputs** for dynamic variable embedding.

    <CodeGroup>
    ```python example.py
    from crewai import Agent

    # Define input variables for prompt templates
    input_variable = "character"
    input_value = "a secret agent"

    # Assign the Helicone-enabled LLM to a Crew AI Agent
    coder = Agent(
        role="Software developer",
        goal=f"Write a story about <helicone-prompt-input key=\"{input_variable}\">{input_value}</helicone-prompt-input>",
        backstory="An expert coder with a keen eye for software trends.",
        llm=helicone_llm
    )
    ```
    </CodeGroup>

    <Note>
      - When using **prompt templates**, ensure your variables appear in the <code>goal</code>, <code>role</code>, or <code>backstory</code> strings.  
      - Refer to [Helicone's Prompt Management docs](https://docs.helicone.ai/features/prompts) for more details on properly structuring prompt inputs.  
      - Explore all available headers and their features in the [Helicone Header Directory](https://docs.helicone.ai/helicone-headers/header-directory).
    </Note>
  </Step>

  <Step title="5. Test and Validate the Integration">
    1. **Run your Crew AI application**: Execute the code to ensure that requests are being routed through Helicone.
    2. **Check Helicone Dashboard**: Navigate to your Helicone dashboard to see if your calls, sessions, and prompt IDs are visible and properly tracked.
    3. **Review Logs & Metrics**: Use Helicone’s analytics to monitor usage, identify bottlenecks, or debug issues in your multi-agent workflow.

    <Note>
      Remember that precise naming conventions, such as `Helicone-Prompt-Id`, help you quickly identify specific prompt runs and agent interactions in Helicone’s logs.
    </Note>
  </Step>
</Steps>

## Additional Information

- For more examples and best practices, explore the [Crew AI GitHub repository](https://github.com/joaomdmoura/crewAI).  
- Leverage Helicone’s observability features like caching, session management, and metrics to optimize your multi-agent workflows.  